\documentclass[5pt,a4paper]{article}

\usepackage{amsmath} % Essential math and alignment (use `&` to align operators')
% Google for more info
\usepackage{amssymb} % Essential symbols for sets and stuff

\usepackage[margin=0.5in]{geometry} % Essential document config

\usepackage{multicol} % Imports the column
\setlength{\columnseprule}{0.4pt} % Divide size

\begin{document}
\begin{multicols}{3}

\section{Week 3}

\begin{itemize}
\item If $f(n) = O(g(n))$, there exist constants $c_1$ and $c_2$ such that $f(n) \leqslant c_2 \cdot g(n)$ holds for all $n \geqslant c_2$.
\item If $f(n) = O(g(n))$, we have $\lim_{n \to \infty} \dfrac{f_1(n)}{g_1(n)} = c$ for some constant $c$. 
\end{itemize}

\section{Week 3 - Extra}

\begin{itemize}
\item When using `Direction 1: Constant Finding' setting $c_1$, always set it to match the coefficent on the LHS so that you can cancel.
\item When trying to get a contradiction, try and isolate an $x \cdot c_1$ on the RHS, where $x \in \mathbb{Z}$, such that an expression that contains $n$ is $\leqslant x \cdot c_1$
\item Make judicious use of the $max$ function when adding functions together
\item If $f_1(n) + f_2(n) \leqslant c_1 \cdot g_1(n) +c'_1 \cdot g_2(n) \leqslant max\{c_1 , c'_1 \} \cdot (g_1(n) + g_2(n))$, for all $n \geqslant max\{c_2, c'_2\}$.
\end{itemize}

\section{Week 4}
\subsection{The Master Theorem}

Let $f(n)$ be a function that returns a positive value for every integer $n>0$. We know:

\begin{align*}
f(1) & \leqslant c_1\\
f(n) & \leqslant \alpha \cdot f(\lceil n / \beta \rceil) + c_2 \cdot n^{\gamma} \text{ for } n \geqslant 2
\end{align*}

where $\alpha, \beta, \gamma, c_1$ and $c_2$ are positive constants. Then:

\begin{itemize}
\item If $log_{b} \alpha < \gamma$ then $f(n) = O(n^\gamma)$
\item If $log_{b} \alpha = \gamma$ then $f(n) = O(n^\gamma log(n))$
\item If $log_{b} \alpha > \gamma$ then $f(n) = O(n^{log_\beta(a)})$

\end{itemize}
\section{Week 5}

TODO

\section{RAM Model}
\subsection{Memory}
Infinite sequence of cells, contains $w$ bits. Every cell has an address starting at 1
\subsection{CPU}
32 registers of width $w$ bits.
\subsubsection{Operations}
Set value to register (constant or from other register). Take two integers from other registers and store the result of; $a+b$, $a-b$, $a\cdot b$, $a/b$. Take two registers and compare them; $a<b$, $a=b$, $a>b$. Read and write from memory.
\subsection{Definitions}
An algorithm is a set of atomic operations. It's cost is is the number of atomic operations. A word is a sequence of $w$ bits
\section{Worst-case}
Worst-case cost of an algorithm is the longest possible running time of input size $n$
\section{Dictionary search}
let $n$ be register 1, and $v$ be register 2\\
register $left\rightarrow1$, $right\rightarrow1$\\
while $left\leq right$\\
\indent register $mid\rightarrow(left+right)/2$\\
\indent if the memory cell at address $mid=v$ then\\
\indent\indent return yes\\
\indent else if memory cell at address $mid>v$ then\\
\indent\indent $right=mid-1$\\
\indent else\\
\indent\indent $left=mid+1$\\
return no\\\\
Worst-case time: $f_2(n)=2+6\log_2n$

\section{Big-O}
We say that $f(n)$ grows asymptotically no faster than $g(n)$ if there is a constant $c_1>0$ such that $f(n)\leq c_1\cdot g(n)$ and holds for all $n$ at least a constant $c_2$. This is denoted by $f(n)=O(g(n))$.
\subsection{Example}
$1000\log_2n=O(n)$,$n\neq O(10000\log_2n)$\\
$\log_{b_1}n=O(\log_{b_2}n)$ for any constants $b_1>1$ and $b_2>1$.
Therefore $f(n)=2+6\log_2n$ can be represented; $f(n)=O(\log n)$
\section{Big-$\Omega$}
If $g(n)=O(f(n))$, then $f(n)=\Omega(g(n))$ to indicate that $f(n)$ grows asymptotically no slower than $g(n)$. We say that $f(n)$ grows asymptotically no slower than $g(n)$ if $c_1>0$ such $f(n)\geq c_1\cdot g(n)$ for $n>c_2$; denoted by $f(n)=\Omega(g(n))$
\section{Big-$\Theta$}
If $f(n)=O(g(n))$ and $f(n)=\Omega(g(n))$, then $f(n)=\Theta(g(n))$ to indicate that $f(n)$ grows asymptotically as fast as $g(n)$
\section{Sort}
\subsection{Merge Sort}
Divide the array into two parts, sort the individual arrays then combine the arrays together.
$f(n)=O(n\log n)$.\\
This is the fastest sorting time possible (apart from $O(n\log\log n)$
\subsection{Counting Sort}
A set S of n integers and every integer is in the range [1, U]. (all integers are distinct)\\
\textbf{Step 1:} Let A be the array storing S. Create array B of length U. Set B to zero.\\
\textbf{Step 2:} For $i\in [1,n]$; Set x to A[i], Set B[x] = 1\\
\textbf{Step 3:} Clear A, For $x\in[1,U]$; If B[x] = 0 continue, otherwise append x to A
\subsubsection{Analysis}
Step 1 and 3 take O(U) time, while Step 2 O(n) time. Therefore running time is O(n + U) = O(U).

\section{Random}
RANDOM(x, y) returns an integer between x and y chosen uniformly at random


\end{multicols}

\end{document}

